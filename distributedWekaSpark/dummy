
     //Dummy test-suite
     
      
          var my_nom2=new ArrayList[String](2)
     my_nom2.add("low")
     my_nom2.add("high")
    val att=new Attribute("total",my_nom2)
      
      
     
      ///Input Parameters 
      val master=options.getMaster
      val hdfsPath=options.getHdfsDatasetInputPath
      val numberOfPartitions=options.getNumberOfPartitions
      val numberOfAttributes=options.getNumberOfAttributes
      val classifierToTrain=options.getClassifier //this must done in-Weka somehow
      val metaL=options.getMetaLearner  //default is weka.classifiers.meta.Vote
    //  val classAtt=options.getClassIndex
      val randomChunks=options.getNumberOfRandomChunks
      var names=new ArrayList[String]
      val folds=options.getNumFolds
      val headerJobOptions=Utils.splitOptions("-N first-last")
       
      val namesPath=options.getNamesPath
      
      
      
     //val cf=new CSVToARFFHeaderMapTask
    
     
      
      

      
      //Load Dataset and cache. ToDo: global caching strategy   -data.persist(StorageLevel.MEMORY_AND_DISK)
       var dataset=hdfshandler.loadRDDFromHDFS(hdfsPath, numberOfPartitions)
       var dataset2=hdfshandler.loadRDDFromHDFS(hdfsPath, 1)
       dataset.persist(options.getCachingStrategy)
       
       
       
       //glom? here on not?
       val namesfromfile=hdfshandler.loadRDDFromHDFS(namesPath,1)
       println(namesfromfile.collect.mkString(""))
   
       
        val names1=new ArrayList[String];for (i <- 0 to 11){names1.add("at"+i)}   
        names=utils.getNamesFromString(namesfromfile.collect.mkString(""))
      //  val names1=utils.getNamesFromString(optionsHandler.getNames.mkString(""))
       //headers
        
        val headerjob=new CSVToArffHeaderSparkJob
        val headers=headerjob.buildHeaders(headerJobOptions,names,numberOfAttributes,dataset)
       // headers.setClassIndex(11)

        println(headers)
     

        
        
        
       
      // var m_rowparser=new CSVToARFFHeaderMapTask()
        
       var dat=dataset.glom.map(new WekaInstancesRDDBuilder(headers).map(_))
       //var dat3=dataset2.glom.map(new WekaInstancesRDDBuilder().map(_,headers))
       var dat2=dataset.glom.map(new WekaInstanceArrayRDDBuilder(headers).map(_))
       dat2.cache
       
       
       
       
      val rulejob=new WekaAssociationRulesSparkJob
      val rules=rulejob.findAssociationRules(dataset,headers,options.getParserOptions,options.getWekaOptions)
      val rulesA=rulejob.findAssociationRules(dat2,headers, options.getParserOptions,options.getWekaOptions)
      val rulesB=rulejob.findAssociationRules(dat,headers, options.getParserOptions,options.getWekaOptions)
      val array=new Array[UpdatableRule](rulesA.keys.size)
      var j=0
      rulesA.foreach{ 
        keyv => 

          array(j)=keyv._2
        j+=1
       }
       Sorting.quickSort(array)
       val fullsupport=new Array[String](array.length)
       val lesssupport=new Array[String](array.length)
       var i=0;var o=0;
       array.foreach{x =>
         x.getTransactions match{
           case  n if n>3000 => fullsupport(i)=x.getRuleString;i+=1
           case _ =>   lesssupport(o)=x.getRuleString;o+=1
         }}
        println("\n Full Support \n")
        fullsupport.foreach{x => if(x!=null)println(x)} 
        println("\n Less support \n")
        lesssupport.foreach{x => if(x!=null)println(x)}
        
          exit(0)
//   
//   exit(0)
//       
//       val classifierfold=new WekaClassifierFoldBasedSparkJob
//       val classf1=classifierfold.buildFoldBasedModel(dataset, headers, folds, classifierToTrain, "default", 11)
//       println("iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii")
//       val classf2=classifierfold.buildFoldBasedModel(dat2, headers, folds, classifierToTrain, "default", 11)
//       println("iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii")
      // val classf3=classifierfold.buildFoldBasedModel(dat, headers, folds, classifierToTrain, "default", 11)
      // println(classf1)
      // println(classf2)
     //  println(classf3)
//       val evalJ1=new WekaClassifierEvaluationSparkJob
//       val e11=evalJ1.evaluateClassifier(classf1, headers, dataset, 11)
//       val e21=evalJ1.evaluateClassifier(classf2, headers, dat2, 11)
//       evalJ1.displayEval(e11)
//       evalJ1.displayEval(e21)
//       
//       exit(0)
       
       val classifierj=new WekaClassifierSparkJob
       
      // val classu=classifierj.buildClassifier(dat,"weka.classifiers.meta.Bagging", classifierToTrain,  headers,  null, null)
       val classu1=classifierj.buildClassifier(dat2,"default", classifierToTrain,  headers,  null, null)
       val classu2=classifierj.buildClassifier(dataset, "default", classifierToTrain, headers,  null, null)
      // println(classu)
       println(classu1)
       println(classu2)
       
       val evalJ=new WekaClassifierEvaluationSparkJob
       val e1=evalJ.evaluateClassifier(classu1, headers, dataset, 11,options.getParserOptions)
       val e2=evalJ.evaluateClassifier(classu2, headers, dat2, 11,options.getParserOptions)
     //  val e3=evalJ.evaluateClassifier(classu, headers, dat, 11)
       println(evalJ.displayEval(e1))
       println(evalJ.displayEval(e2))
      // println(evalJ.displayEval(e3))
       val ttest=new NaiveBayes
      // ttest.buildClassifier(dat3.first)
       
     //  val e4=new Evaluation(dat3.first)
      // e4.evaluateModel(x$1, x$2, x$3)
     //  e4.evaluateModel(ttest, dat3.first, null)
      // evalJ.displayEval(e4)
       
       
       exit(0)
       
       
      // var classifier=dat.map(x=> new TestClassifierMapper().map(x)).collect
      // classifier.foreach{x => println(x)}
     
       
       
    // exit(0)
      // hdfshandler.saveToHDFS(headers, "user/weka/testhdfs.txt", "testtext")
       
        // hdfshandler.saveObjectToHDFS(headers, "hdfs://sandbox.hortonworks.com:8020/user/weka/", null)
      //   val h=hdfshandler.loadObjectFromHDFS("hdfs://sandbox.hortonworks.com:8020/user/weka/")
        // val h2=new Instances(h)

         
       //randomize if necessary 
      // if(randomChunks>0){dataset=new WekaRandomizedChunksSparkJob().randomize(dataset, randomChunks, headers, classAtt)}
       
     //build foldbased
//      val foldjob=new WekaClassifierFoldBasedSparkJob
//      val classifier=foldjob.buildFoldBasedModel(dataset, headers, folds, classifierToTrain, metaL,classAtt)
//      println(classifier.toString())
//      val evalfoldjob=new WekaClassifierEvaluationSparkJob
//      val eval=evalfoldjob.evaluateFoldBasedClassifier(folds, classifier, headers, dataset,classAtt)
//      evalfoldjob.displayEval(eval)
  
//      //build a classifier+ evaluate
      val classifierjob=new WekaClassifierSparkJob
      val classifier2=classifierjob.buildClassifier(dataset,metaL,classifierToTrain,headers,null,options.getWekaOptions) 
      println(classifier2)
//      val evaluationJob=new WekaClassifierEvaluationSparkJob
//      val eval2=evaluationJob.evaluateClassifier(classifier2, headers, dataset,classAtt)
//
//      println(classifier2.toString())
//      evaluationJob.displayEval(eval2)
    
      //val broad=sc.broadcast(headers)
      
      exit(0)